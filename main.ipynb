{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b8a061df",
      "metadata": {
        "id": "b8a061df"
      },
      "source": [
        "# Importing Libraries\n",
        "\n",
        "In this section, we import the necessary libraries required for data processing, API requests, multithreading, and logging. These libraries include:\n",
        "\n",
        "- `concurrent.futures` for managing threads.\n",
        "- `datetime` for handling date and time operations.\n",
        "- `pandas` for data manipulation and analysis.\n",
        "- `requests` for making HTTP requests.\n",
        "- `dotenv` for loading environment variables from `.env` files.\n",
        "- `json` for working with JSON data.\n",
        "- `os` for interacting with the operating system.\n",
        "- `time` for time-related functions.\n",
        "- `logging` for implementing logging functionality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c65210a4",
      "metadata": {
        "id": "c65210a4"
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import logging\n",
        "from datetime import datetime\n",
        "# Third-party library imports\n",
        "import pandas as pd\n",
        "import requests\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import dotenv  # For loading environment variables"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55d45e97",
      "metadata": {
        "id": "55d45e97"
      },
      "source": [
        "# Configuring Logging\n",
        "\n",
        "In this section, we set up the logging configuration to track the application's execution. This includes:\n",
        "\n",
        "- Creating a directory for storing log files.\n",
        "- Generating a log file with a timestamped filename.\n",
        "- Configuring both file and console handlers for logging.\n",
        "- Formatting log messages with timestamps and log levels.\n",
        "- Ensuring no duplicate log entries are propagated.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d8fedad7",
      "metadata": {
        "id": "d8fedad7"
      },
      "outputs": [],
      "source": [
        "# Create a directory for logs if it doesn't exist\n",
        "log_dir = \"logs\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Generate a log filename with a timestamp\n",
        "log_filename = f\"{log_dir}/scraping_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
        "\n",
        "# Configure the logging settings\n",
        "logger = logging.getLogger(__name__)  # Use __name__ for modular logging\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# File handler for logging to a file\n",
        "file_handler = logging.FileHandler(log_filename, mode='w')\n",
        "file_handler.setLevel(logging.INFO)\n",
        "file_formatter = logging.Formatter('[%(asctime)s] %(levelname)s - %(message)s')\n",
        "file_handler.setFormatter(file_formatter)\n",
        "logger.addHandler(file_handler)\n",
        "\n",
        "# Console handler for logging to the console\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setLevel(logging.INFO)\n",
        "console_formatter = logging.Formatter('[%(asctime)s] %(levelname)s - %(message)s')\n",
        "console_handler.setFormatter(console_formatter)\n",
        "logger.addHandler(console_handler)\n",
        "\n",
        "# Avoid duplicate log entries\n",
        "logger.propagate = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3a0e3fc",
      "metadata": {
        "id": "d3a0e3fc"
      },
      "source": [
        "# Loading Access Tokens and Saving Scraped Repository Data\n",
        "\n",
        "This section explains the workflow for loading access tokens, scraping GitHub repository data, and saving the data into a CSV file. Below is a detailed breakdown of the steps:\n",
        "\n",
        "1. **Load Access Tokens**:\n",
        "\n",
        "   - The `load_access_tokens` function retrieves the required access tokens from a `.env` file using the `dotenv` library.\n",
        "   - These tokens are essential for authenticating API requests to GitHub.\n",
        "\n",
        "2. **Scrape GitHub Repositories**:\n",
        "\n",
        "   - The `scrape_github_repositories` function performs the task of fetching repository data from GitHub based on predefined search queries.\n",
        "   - It handles pagination, rate-limiting, and error logging to ensure robust data collection.\n",
        "\n",
        "3. **Save Repository Data to CSV**:\n",
        "   - The `save_repositories_to_csv` function processes the scraped repository data and saves it into a CSV file.\n",
        "   - This file can be used for further analysis or reporting.\n",
        "\n",
        "These steps ensure a streamlined process for collecting and storing GitHub repository data efficiently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ebd689a9",
      "metadata": {
        "id": "ebd689a9"
      },
      "outputs": [],
      "source": [
        "def load_access_tokens():\n",
        "    \"\"\"\n",
        "    Load access tokens from the .env file using the dotenv library.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing access_token_1 and access_token_2.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If one or both access tokens are missing in the .env file.\n",
        "    \"\"\"\n",
        "    dotenv.load_dotenv()  # Load environment variables from the .env file\n",
        "    access_token_1 = os.getenv('token_1')\n",
        "    access_token_2 = os.getenv('token_2')\n",
        "\n",
        "    # Check if both tokens are loaded successfully\n",
        "    if not access_token_1 or not access_token_2:\n",
        "        logger.error(\"One or both access tokens are missing in the .env file.\")\n",
        "        raise ValueError(\"One or both access tokens are missing in the .env file.\")\n",
        "\n",
        "    logger.info(\"Access tokens loaded successfully.\")\n",
        "    return access_token_1, access_token_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a6b4378f",
      "metadata": {
        "id": "a6b4378f"
      },
      "outputs": [],
      "source": [
        "def scrape_github_repositories(\n",
        "    search_queries: list[str],\n",
        "    access_token: str,\n",
        "    pages: int = 10\n",
        ") -> list[dict]:\n",
        "    \"\"\"\n",
        "    Scrape GitHub repositories based on search queries.\n",
        "\n",
        "    Args:\n",
        "        search_queries (list[str]): List of search queries.\n",
        "        access_token (str): GitHub access token for authentication.\n",
        "        pages (int): Number of pages to scrape per query (default is 10).\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: List of scraped repositories.\n",
        "    \"\"\"\n",
        "    repositories_list = []\n",
        "    headers = {'Authorization': f'token {access_token}'}\n",
        "\n",
        "    for query in search_queries:\n",
        "        logger.info(f'Starting to scrape repositories for query: \"{query}\"...')\n",
        "        for page in range(1, pages + 1):\n",
        "            payload = {'q': query, 'per_page': 100, 'page': page}\n",
        "            url = \"https://api.github.com/search/repositories\"\n",
        "\n",
        "            try:\n",
        "                response = requests.get(url, headers=headers, params=payload)\n",
        "                response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "\n",
        "                items = response.json().get('items', [])\n",
        "                if items:\n",
        "                    repositories_list.extend(items)\n",
        "                    logger.info(f'Page {page} done! Scraped {len(items)} repositories.')\n",
        "                else:\n",
        "                    logger.info(f'No repositories found for query: \"{query}\" on page {page}.')\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                logger.error(f'Error on page {page} for query \"{query}\": {e}')\n",
        "                break  # Stop processing this query if an error occurs\n",
        "\n",
        "            # Handle rate limiting\n",
        "            remaining = int(response.headers.get(\"X-RateLimit-Remaining\", 0))\n",
        "            if remaining == 0:\n",
        "                reset_time = int(response.headers.get(\"X-RateLimit-Reset\", 0))\n",
        "                sleep_duration = reset_time - time.time() + 1\n",
        "                logger.info(f'Rate limit exceeded. Sleeping for {sleep_duration:.0f} seconds...')\n",
        "                time.sleep(max(sleep_duration, 0))\n",
        "\n",
        "    logger.info('Scraping completed.')\n",
        "    return repositories_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "14f4ea39",
      "metadata": {
        "id": "14f4ea39"
      },
      "outputs": [],
      "source": [
        "def save_repositories_to_csv(\n",
        "        repositories: list,\n",
        "        output_file: str ='repositories_data.csv'\n",
        "        ) -> None:\n",
        "    \"\"\"\n",
        "    Save repository data to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        repositories (list[dict]): List of repository data dictionaries.\n",
        "        output_file (str): Name of the output CSV file (default is 'repositories_data.csv').\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    if not repositories:\n",
        "        logger.warning(\"No repositories to save. The repositories list is empty.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Extract relevant fields from each repository\n",
        "        repository_records = [\n",
        "            {\n",
        "                'Repository Name': repo.get('name', 'N/A'),\n",
        "                'Owner Username': repo.get('owner', {}).get('login', 'N/A'),\n",
        "                'Owner Profile URL': repo.get('owner', {}).get('html_url', 'N/A'),\n",
        "                'Owner Type': repo.get('owner', {}).get('type', 'N/A'),\n",
        "                'Description': repo.get('description', 'N/A'),\n",
        "                'Created Date': repo.get('created_at', 'N/A'),\n",
        "                'Updated Date': repo.get('updated_at', 'N/A'),\n",
        "                'Last Pushed Date': repo.get('pushed_at', 'N/A'),\n",
        "                'Repository Size (KB)': repo.get('size', 'N/A'),\n",
        "                'Stars Count': repo.get('stargazers_count', 0),\n",
        "                'Watchers Count': repo.get('watchers_count', 0),\n",
        "                'Primary Language': repo.get('language', 'N/A'),\n",
        "                'Forks Count': repo.get('forks_count', 0),\n",
        "                'Open Issues Count': repo.get('open_issues_count', 0),\n",
        "                'Topics': ', '.join(repo.get('topics', [])),\n",
        "                'Default Branch': repo.get('default_branch', 'N/A')\n",
        "            }\n",
        "            for repo in repositories\n",
        "        ]\n",
        "\n",
        "        # Convert to DataFrame and save to CSV\n",
        "        df = pd.DataFrame(repository_records)\n",
        "        df.to_csv(output_file, index=False, encoding='utf-8')\n",
        "        logger.info(f\"Repository data successfully saved to '{output_file}'.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred while saving repositories to CSV: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73c8a7e9",
      "metadata": {
        "id": "73c8a7e9"
      },
      "source": [
        "# Scraped Users and Organizations Data\n",
        "\n",
        "In this section, we analyze the scraped repository data to extract unique individual users and organizations. The extracted data is then saved into separate CSV files for further analysis or reporting.\n",
        "\n",
        "Below is an explanation of the subsequent cells:\n",
        "\n",
        "1. **Analyze Users and Organizations**:\n",
        "\n",
        "   - The `analyze_users_and_organizations` function reads the repository data from a CSV file.\n",
        "   - It identifies and separates unique individual users and organizations based on the \"Owner Type\" field.\n",
        "\n",
        "2. **Fetch User Details**:\n",
        "\n",
        "   - The `fetch_user_details` function retrieves detailed information about a specific GitHub user using the GitHub API.\n",
        "\n",
        "3. **Fetch Organization Details**:\n",
        "\n",
        "   - The `fetch_organization_data` function retrieves detailed information about a specific GitHub organization using the GitHub API.\n",
        "\n",
        "4. **Save Users Data to CSV**:\n",
        "\n",
        "   - The `save_users_data_to_csv` function saves the detailed user data into a CSV file.\n",
        "\n",
        "5. **Save Organizations Data to CSV**:\n",
        "   - The `save_organizations_data_to_csv` function saves the detailed organization data into a CSV file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e2feaff1",
      "metadata": {
        "id": "e2feaff1"
      },
      "outputs": [],
      "source": [
        "def analyze_users_and_organizations(\n",
        "        csv_file: str\n",
        "        ) -> tuple[list[str], list[str]]:\n",
        "    \"\"\"\n",
        "    Analyze the CSV file to extract unique individual users and organizations.\n",
        "\n",
        "    Args:\n",
        "        csv_file (str): Path to the CSV file containing repository data.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two lists - unique individual users and unique organizations.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the CSV file into a DataFrame\n",
        "        df = pd.read_csv(csv_file)\n",
        "\n",
        "        # Extract unique users and organizations\n",
        "        unique_users = df[df[\"Owner Type\"] == 'User']['Owner Username'].dropna().unique().tolist()\n",
        "        unique_organizations = df[df[\"Owner Type\"] == 'Organization']['Owner Username'].dropna().unique().tolist()\n",
        "\n",
        "        # Log the results\n",
        "        logger.info(f'Total Individual Users: {len(unique_users)}')\n",
        "        logger.info(f'Total Organizations: {len(unique_organizations)}')\n",
        "\n",
        "        return unique_users, unique_organizations\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        logger.error(f\"The file '{csv_file}' was not found.\")\n",
        "        return [], []\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred while analyzing the file '{csv_file}': {e}\")\n",
        "        return [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bc48299d",
      "metadata": {
        "id": "bc48299d"
      },
      "outputs": [],
      "source": [
        "def fetch_user_details(\n",
        "        username: str,\n",
        "        token: str\n",
        "        ) -> dict:\n",
        "    \"\"\"\n",
        "    Fetch detailed information about a GitHub user.\n",
        "\n",
        "    Args:\n",
        "        username (str): GitHub username.\n",
        "        token (str): GitHub access token for authentication.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: A dictionary containing user details if successful, otherwise None.\n",
        "    \"\"\"\n",
        "    url = f\"https://api.github.com/users/{username}\"\n",
        "    headers = {'Authorization': f'token {token}'}\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "\n",
        "        user_data = response.json()\n",
        "        user_info = {\n",
        "            'Username': user_data.get('login', 'N/A'),\n",
        "            'User ID': user_data.get('id', 'N/A'),\n",
        "            'Profile URL': user_data.get('html_url', 'N/A'),\n",
        "            'Name': user_data.get('name', 'N/A'),\n",
        "            'Company': user_data.get('company', 'N/A'),\n",
        "            'Location': user_data.get('location', 'N/A'),\n",
        "            'Bio': user_data.get('bio', 'N/A'),\n",
        "            'Blog': user_data.get('blog', 'N/A'),\n",
        "            'Twitter Username': user_data.get('twitter_username', 'N/A'),\n",
        "            'Email': user_data.get('email', 'N/A'),\n",
        "            'Public Repos': user_data.get('public_repos', 0),\n",
        "            'Public Gists': user_data.get('public_gists', 0),\n",
        "            'Followers': user_data.get('followers', 0),\n",
        "            'Following': user_data.get('following', 0),\n",
        "            'Account Created': user_data.get('created_at', 'N/A'),\n",
        "            'Last Updated': user_data.get('updated_at', 'N/A')\n",
        "        }\n",
        "        logger.info(f\"Fetched data for user '{username}' successfully.\")\n",
        "        return user_info\n",
        "\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        logger.error(f\"HTTP error occurred while fetching data for user '{username}': {http_err}\")\n",
        "    except requests.exceptions.RequestException as req_err:\n",
        "        logger.error(f\"Request exception occurred for user '{username}': {req_err}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An unexpected error occurred for user '{username}': {e}\")\n",
        "\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "70ea5e07",
      "metadata": {
        "id": "70ea5e07"
      },
      "outputs": [],
      "source": [
        "def fetch_organization_data(\n",
        "        org_username: str,\n",
        "        token: str\n",
        "        ) -> dict:\n",
        "    \"\"\"\n",
        "    Fetch detailed information about a GitHub organization.\n",
        "\n",
        "    Args:\n",
        "        org_username (str): GitHub organization username.\n",
        "        token (str): GitHub access token for authentication.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: A dictionary containing organization details if successful, otherwise None.\n",
        "    \"\"\"\n",
        "    url = f\"https://api.github.com/users/{org_username}\"\n",
        "    headers = {'Authorization': f'token {token}'}\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "\n",
        "        org_data = response.json()\n",
        "        org_info = {\n",
        "            'Organization Username': org_data.get('login', 'N/A'),\n",
        "            'Organization ID': org_data.get('id', 'N/A'),\n",
        "            'Profile URL': org_data.get('html_url', 'N/A'),\n",
        "            'Full Name': org_data.get('name', 'N/A'),\n",
        "            'Location': org_data.get('location', 'N/A'),\n",
        "            'Bio': org_data.get('bio', 'N/A'),\n",
        "            'Blog URL': org_data.get('blog', 'N/A'),\n",
        "            'Twitter Username': org_data.get('twitter_username', 'N/A'),\n",
        "            'Email': org_data.get('email', 'N/A'),\n",
        "            'Public Repositories': org_data.get('public_repos', 0),\n",
        "            'Public Gists': org_data.get('public_gists', 0),\n",
        "            'Followers Count': org_data.get('followers', 0),\n",
        "            'Following Count': org_data.get('following', 0),\n",
        "            'Account Created At': org_data.get('created_at', 'N/A'),\n",
        "            'Last Updated At': org_data.get('updated_at', 'N/A')\n",
        "        }\n",
        "        logger.info(f\"Fetched data for organization '{org_username}' successfully.\")\n",
        "        return org_info\n",
        "\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        logger.error(f\"HTTP error occurred while fetching data for organization '{org_username}': {http_err}\")\n",
        "    except requests.exceptions.RequestException as req_err:\n",
        "        logger.error(f\"Request exception occurred for organization '{org_username}': {req_err}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An unexpected error occurred for organization '{org_username}': {e}\")\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8d6de6a4",
      "metadata": {
        "id": "8d6de6a4"
      },
      "outputs": [],
      "source": [
        "def save_users_data_to_csv(\n",
        "        users_data: list[dict],\n",
        "        file_name: str='users_data.csv'\n",
        "        ) -> None:\n",
        "    \"\"\"\n",
        "    Save user data to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        users_data (list[dict]): List of user data dictionaries.\n",
        "        file_name (str): Name of the output CSV file (default is 'users_data.csv').\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    if not users_data:\n",
        "        logger.info(\"No user data to save. The users_data list is empty.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Convert the list of dictionaries to a DataFrame\n",
        "        df = pd.DataFrame(users_data)\n",
        "        df.fillna('N/A', inplace=True)  # Replace NaN values with 'N/A'\n",
        "\n",
        "        # Save the DataFrame to a CSV file\n",
        "        df.to_csv(file_name, index=False, encoding='utf-8')\n",
        "        logger.info(f\"User data successfully saved to '{file_name}'.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred while saving user data to CSV: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c8ff07ed",
      "metadata": {
        "id": "c8ff07ed"
      },
      "outputs": [],
      "source": [
        "def save_organizations_data_to_csv(\n",
        "    organizations_data: list[dict],\n",
        "    file_name: str = 'organizations_data.csv'\n",
        "    ) -> None:\n",
        "    \"\"\"\n",
        "    Save organization data to a CSV file.\n",
        "\n",
        "    Args:\n",
        "    organizations_data (list[dict]): List of organization data dictionaries.\n",
        "    file_name (str): Name of the output CSV file (default is 'organizations_data.csv').\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    if not organizations_data:\n",
        "        logger.info(\"No organization data to save. The organizations_data list is empty.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Convert the list of dictionaries to a DataFrame\n",
        "        df = pd.DataFrame(organizations_data)\n",
        "        df.fillna('N/A', inplace=True)  # Replace NaN values with 'N/A'\n",
        "\n",
        "        # Save the DataFrame to a CSV file\n",
        "        df.to_csv(file_name, index=False, encoding='utf-8')\n",
        "        logger.info(f\"Organization data successfully saved to '{file_name}'.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred while saving organization data to CSV: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0680fd4",
      "metadata": {
        "id": "a0680fd4"
      },
      "source": [
        "# Main Function to Execute the Workflow\n",
        "\n",
        "The `main` function orchestrates the entire workflow by sequentially calling the previously defined methods. It ensures that the process of loading access tokens, scraping GitHub repositories, analyzing users and organizations, and saving the data is executed in a streamlined manner.\n",
        "\n",
        "### Workflow Steps:\n",
        "\n",
        "1. **Load Access Tokens**:\n",
        "\n",
        "   - The function begins by loading the required GitHub access tokens from the `.env` file using the `load_access_tokens` method.\n",
        "   - These tokens are essential for authenticating API requests.\n",
        "\n",
        "2. **Define Search Queries**:\n",
        "\n",
        "   - A list of predefined search queries is used to scrape repositories related to various domains such as front-end, back-end, data science, etc.\n",
        "\n",
        "3. **Scrape Repositories**:\n",
        "\n",
        "   - The `scrape_github_repositories` method fetches repository data from GitHub based on the search queries.\n",
        "   - The scraped data is saved into a CSV file using the `save_repositories_to_csv` method.\n",
        "\n",
        "4. **Analyze Users and Organizations**:\n",
        "\n",
        "   - The `analyze_users_and_organizations` method processes the repository data to extract unique individual users and organizations.\n",
        "\n",
        "5. **Fetch User Details**:\n",
        "\n",
        "   - Using multithreading, the `fetch_user_details` method retrieves detailed information about each user.\n",
        "   - The data is saved into a CSV file using the `save_users_data_to_csv` method.\n",
        "\n",
        "6. **Fetch Organization Details**:\n",
        "\n",
        "   - Similarly, the `fetch_organization_data` method retrieves detailed information about each organization using multithreading.\n",
        "   - The data is saved into a CSV file using the `save_organizations_data_to_csv` method.\n",
        "\n",
        "7. **Logging and Error Handling**:\n",
        "   - Throughout the workflow, logging is used to track progress and handle errors gracefully.\n",
        "\n",
        "This function ensures that all the steps are executed in the correct order, providing a comprehensive solution for scraping and analyzing GitHub repository data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c49deca1",
      "metadata": {
        "id": "c49deca1"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    try:\n",
        "        # Load access tokens\n",
        "        access_token_1, access_token_2 = load_access_tokens()\n",
        "        logger.info(\"Access tokens loaded successfully.\")\n",
        "    except ValueError as e:\n",
        "        logger.error(f\"Error loading access tokens: {e}\")\n",
        "        return\n",
        "\n",
        "    # Define search queries\n",
        "    search_queries = [\n",
        "        'front-end', 'back-end', 'full-stack', 'web-development',\n",
        "        'mobile-development', 'data-science', 'machine-learning',\n",
        "        'artificial-intelligence', 'cloud-computing', 'cybersecurity'\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        # Scrape repositories and save to CSV\n",
        "        repositories_data = scrape_github_repositories(search_queries, access_token_1, pages=10)\n",
        "        save_repositories_to_csv(repositories_data, 'repositories_data.csv')\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during repository scraping or saving: {e}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Analyze users and organizations\n",
        "        users, organizations = analyze_users_and_organizations('repositories_data.csv')\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error analyzing users and organizations: {e}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Fetch user details using multithreading\n",
        "        with ThreadPoolExecutor(max_workers=100) as executor:\n",
        "            future_tasks = [\n",
        "                executor.submit(fetch_user_details, username, access_token_1 if i < 4000 else access_token_2)\n",
        "                for i, username in enumerate(users)\n",
        "            ]\n",
        "            all_user_details = [future.result() for future in future_tasks if future.result() is not None]\n",
        "        save_users_data_to_csv(all_user_details, 'users_data.csv')\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching or saving user details: {e}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Fetch organization details using multithreading\n",
        "        with ThreadPoolExecutor(max_workers=100) as executor:\n",
        "            future_tasks = [\n",
        "                executor.submit(fetch_organization_data, org_username, access_token_2)\n",
        "                for org_username in organizations\n",
        "            ]\n",
        "            all_organizations_data = [future.result() for future in future_tasks if future.result() is not None]\n",
        "        save_organizations_data_to_csv(all_organizations_data, 'organizations_data.csv')\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching or saving organization details: {e}\")\n",
        "        return\n",
        "\n",
        "    logger.info(\"Data scraping and saving completed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac6fcc40",
      "metadata": {
        "id": "ac6fcc40"
      },
      "source": [
        "# Running the Main Function on `__name__ == \"__main__\"`\n",
        "\n",
        "This section ensures that the `main` function is executed only when the script is run directly, and not when it is imported as a module. This is achieved using the `if __name__ == \"__main__\":` condition, which is a common Python convention for defining the entry point of a program.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "241f27df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "241f27df",
        "outputId": "5ad3775c-8c2d-47a2-a201-64f820a3fe1d"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Starting the GitHub scraping workflow...\")\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
