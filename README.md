# ğŸ•¸ï¸ Web Scraping Pipeline with Logging and Multithreading

This project is a robust and modular web scraping pipeline built in Python. It leverages `requests` for HTTP interactions, `pandas` for data processing, `ThreadPoolExecutor` for multithreading, and a custom logging system for detailed monitoring. Configuration values are handled securely using environment variables via the `.env` file.

---

## ğŸ“Œ Features

- âœ… Multi-threaded scraping with `concurrent.futures`
- âœ… Structured and timestamped logging to files and console
- âœ… Data extraction and transformation using `pandas`
- âœ… Environment variable support using `dotenv`
- âœ… Organized codebase and reproducible results

---

## ğŸ—‚ï¸ Project Structure

```
.
â”œâ”€â”€ main.ipynb           # Jupyter Notebook for running and modifying the pipeline
â”œâ”€â”€ logs/                # Folder for log files (auto-created)
â”œâ”€â”€ .env                 # Environment variables (you must create this)
â”œâ”€â”€ requirements.txt     # Required Python libraries
â””â”€â”€ README.md            # Project documentation
```

---

## âš™ï¸ Requirements

- Python 3.8 or higher
- pip (Python package installer)
- Internet connection for making HTTP requests

---

## ğŸ“¥ Installation

1. **Clone the repository or download the project files:**

```bash
git clone https://github.com/yourusername/yourproject.git
cd yourproject
```

2. **Create and activate a virtual environment:**

```bash
python -m venv venv
# On Unix/macOS:
source venv/bin/activate
# On Windows:
venv\Scripts\activate
```

3. **Install dependencies:**

```bash
pip install -r requirements.txt
```

---

## ğŸ” Environment Variables

Create a `.env` file in the project root to securely store any sensitive information or configuration parameters (like URLs, tokens, etc.). Example:

```env
token_1 = "your first access token"
token_2 = "your second access token"
```

> Ensure your `.env` file is excluded from version control using `.gitignore`.

---

## ğŸš€ Usage

1. Open `main.ipynb` using Jupyter Notebook or JupyterLab.
2. Run the cells sequentially to:
   - Load environment variables
   - Initialize logging
   - Perform concurrent web scraping
   - Process and store the scraped data

---

## ğŸ“Š Logging

- Logs are saved automatically in the `logs/` directory.
- Each log file includes a timestamp for easy tracking.
- Both INFO and ERROR logs are written to console and file.

Example log filename:

```
logs/scraping_log_20250509_153045.log
```

---

## ğŸ§ª Dependencies

See `requirements.txt`:

```text
pandas
requests
python-dotenv
```

You can install them manually via:

```bash
pip install pandas requests python-dotenv
```

---

## ğŸ“„ License

This project is open-source and available under the [MIT License](https://opensource.org/licenses/MIT).

---

## ğŸ™‹â€â™‚ï¸ Author

GitHub: [@ahmed-eltayep](https://github.com/ahmed-eltayep)

---

## ğŸ¤ Contributing

Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.
